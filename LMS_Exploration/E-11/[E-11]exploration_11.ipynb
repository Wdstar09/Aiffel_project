{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[E-11]exploration_11.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1So1fPc9RWNoVgUen2BtPWZ-uaVY5I8Ie","authorship_tag":"ABX9TyPhATnotK57VrZgIsorBYSN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# [ EXPLORATION ] 11. 트랜스포머로 만드는 대화형 챗봇\n","\n","✅트랜스포머의 인코더 디코더 구조와 셀프 어텐션을 코드를 통해 이해해 본다. 이를 영어와 한국어로 이루어진 챗봇 데이터에 적용해 본다.\n","\n","---\n","### - 📖목차\n","* ✔️11-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n","* ✔️회고 및 결론\n","* ✔️Reference(참고자료)\n","\n","<br>\n","\n"],"metadata":{"id":"nbkCP6rYCTY5"}},{"cell_type":"markdown","source":["## ✔️11-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n","---\n","### Step 1. 데이터 수집하기\n","\n","* 한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다. 이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n","\n","https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv"],"metadata":{"id":"07tYW8eMCpHz"}},{"cell_type":"code","source":["# 모듈 로드\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import os\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"a5sQ_l2jxymw","executionInfo":{"status":"ok","timestamp":1654964663214,"user_tz":-540,"elapsed":220,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# mecab 설치 + 한뒤에 런타임 필수\n","!apt-get update\n","!apt-get install g++ openjdk-8-jdk \n","!pip3 install konlpy JPype1-py3\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"id":"2paQgeZhpjIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 경고 무시\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"],"metadata":{"id":"KOSo7-SBjtnT","executionInfo":{"status":"ok","timestamp":1654964340656,"user_tz":-540,"elapsed":333,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CLPlVkn7CSyF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654964357478,"user_tz":-540,"elapsed":13622,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"6ba8a1a7-0d5c-4cae-cced-e3cb1c856c41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# 데이터 불러오기(코랩 환경이라 google 마운트를 씁니다)\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# 파일 경로 확인(전체)\n","!ls '/content/gdrive/MyDrive/aiffel/ex_11' #프로젝트에 쓰게될 이미지들을 가져온 상태"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfjVMl2Na17s","executionInfo":{"status":"ok","timestamp":1654964383616,"user_tz":-540,"elapsed":599,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"3278742f-10d1-4073-9a5d-4fa40f300b70"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'ChatbotData .csv'\n"]}]},{"cell_type":"code","source":["# 데이터 확인\n","data = pd.read_csv('/content/gdrive/MyDrive/aiffel/ex_11/ChatbotData .csv')\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"8xt7LWBxk911","executionInfo":{"status":"ok","timestamp":1654964973130,"user_tz":-540,"elapsed":242,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"e6641fc9-72df-49c0-cb6a-b8783a827b5e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                             Q                         A  label\n","0                       12시 땡!                하루가 또 가네요.      0\n","1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n","2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n","3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n","4                      PPL 심하네                눈살이 찌푸려지죠.      0\n","...                        ...                       ...    ...\n","11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n","11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n","11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n","11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n","11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n","\n","[11823 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-30ed2aa8-8c1e-478a-8834-0ded0376cda1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11818</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>티가 나니까 눈치가 보이는 거죠!</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11819</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>훔쳐보는 거 티나나봐요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11820</th>\n","      <td>흑기사 해주는 짝남.</td>\n","      <td>설렜겠어요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11821</th>\n","      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n","      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11822</th>\n","      <td>힘들어서 결혼할까봐</td>\n","      <td>도피성 결혼은 하지 않길 바라요.</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11823 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30ed2aa8-8c1e-478a-8834-0ded0376cda1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-30ed2aa8-8c1e-478a-8834-0ded0376cda1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-30ed2aa8-8c1e-478a-8834-0ded0376cda1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 데이터 갯수 확인\n","print('챗봇 샘플의 개수 :', len(data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ot-H9MS3l6Ek","executionInfo":{"status":"ok","timestamp":1654964975204,"user_tz":-540,"elapsed":237,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"a85b5322-14a0-4c35-b8b8-abd2a8524aac"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["챗봇 샘플의 개수 : 11823\n"]}]},{"cell_type":"code","source":["# 결측치 있는지 확인\n","data.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6rBC5EumNBv","executionInfo":{"status":"ok","timestamp":1654964976213,"user_tz":-540,"elapsed":2,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"61bbb116-41c3-44a4-9755-eb8d156020c5"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Q        0\n","A        0\n","label    0\n","dtype: int64"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["- 챗봇 샘플이 1만 여개 밖에 되지 않는데 이게 학습이 과연 잘될까 초장부터 걱정이 되었다......"],"metadata":{"id":"bTj51CsPmABn"}},{"cell_type":"markdown","source":["<br>\n","\n","### Step 2. 데이터 전처리하기\n","- 영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."],"metadata":{"id":"iX2T8XlfDI6K"}},{"cell_type":"code","source":["# 쓸데없는 column 미리 제거\n","data = data.drop(['label'], axis=1)"],"metadata":{"id":"S7OJgawVDFW6","executionInfo":{"status":"ok","timestamp":1654964984051,"user_tz":-540,"elapsed":349,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 전처리 함수\n","def preprocess_sentence(sentence):\n","  sentence = sentence.lower().strip()\n","\n","  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n","  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n","  # student와 온점 사이에 거리를 만듭니다.\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","\n","  # (한글, 알파벳, 숫자 \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백으로 대체합니다.\n","  sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence) \n","  sentence = sentence.strip()\n","  return sentence"],"metadata":{"id":"NamfxWDHktDc","executionInfo":{"status":"ok","timestamp":1654966076747,"user_tz":-540,"elapsed":681,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n","def load_conversations():\n","    inputs, outputs = [], []\n","\n","# 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.    \n","    for i in range(len(data)):\n","        inputs.append(preprocess_sentence(data['Q'][i]))\n","        outputs.append(preprocess_sentence(data['A'][i]))\n","        \n","    return inputs, outputs"],"metadata":{"id":"WOq5xVAZqdss","executionInfo":{"status":"ok","timestamp":1654966082259,"user_tz":-540,"elapsed":858,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장\n","questions, answers = load_conversations()\n","print('- 전체 질문 수 :', len(questions))\n","print('- 전체 답변 수 :', len(answers))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycLYziQUktAt","executionInfo":{"status":"ok","timestamp":1654966115696,"user_tz":-540,"elapsed":634,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"ab8216ad-4789-4495-a8a7-8250050fe9e8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["- 전체 질문 수 : 11823\n","- 전체 답변 수 : 11823\n"]}]},{"cell_type":"code","source":["print('- 전처리 후의 10번째 질문 샘플: {}'.format(questions[10]))\n","print('- 전처리 후의 10번째 답변 샘플: {}'.format(answers[10]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gY-pP7J0qgpG","executionInfo":{"status":"ok","timestamp":1654966102664,"user_tz":-540,"elapsed":255,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"1cc06649-8ebe-4f8c-97b9-7515a0f67beb"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["- 전처리 후의 10번째 질문 샘플: sns보면 나만 빼고 다 행복해보여\n","- 전처리 후의 10번째 답변 샘플: 자랑하는 자리니까요 .\n"]}]},{"cell_type":"code","source":["print(questions[:5])\n","print(answers[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ffa9QN-ks-E","executionInfo":{"status":"ok","timestamp":1654966120631,"user_tz":-540,"elapsed":232,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"b06b9af6-2f9c-4089-ebb5-f331f1b8a8b3"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'ppl 심하네']\n","['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","\n","### Step 3. SubwordTextEncoder 사용하기\n","- 한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."],"metadata":{"id":"egL7Ym88DMqW"}},{"cell_type":"code","source":["# SubwordTextEncoder를 사용하여 질문, 답변 데이터로부터 단어집합 생성\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)"],"metadata":{"id":"bgCae19gDOTJ","executionInfo":{"status":"ok","timestamp":1654965116042,"user_tz":-540,"elapsed":16587,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 시작 토큰과 종료 토큰에 고유한 정수를 부여\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size+1]\n","VOCAB_SIZE = tokenizer.vocab_size + 2 # 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정\n","print(VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDcL3y_cktn_","executionInfo":{"status":"ok","timestamp":1654966211767,"user_tz":-540,"elapsed":293,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"3c67bab7-bf65-4138-fe9d-d9a2cf452c30"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["8180\n"]}]},{"cell_type":"code","source":["print('- 정수 인코딩 후의 10번째 질문 샘플: {}'.format(tokenizer.encode(questions[11])))\n","print('- 정수 인코딩 후의 10번째 답변 샘플: {}'.format(tokenizer.encode(answers[11])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUuSPUnIq_AV","executionInfo":{"status":"ok","timestamp":1654966232700,"user_tz":-540,"elapsed":234,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"7633f743-76d1-4450-a537-46f1891b1fcb"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["- 정수 인코딩 후의 10번째 질문 샘플: [5766, 611, 2495, 4167]\n","- 정수 인코딩 후의 10번째 답변 샘플: [2359, 7516, 7, 6279, 97, 1]\n"]}]},{"cell_type":"code","source":["# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n","MAX_LENGTH = 40"],"metadata":{"id":"BJVvhEfyktkc","executionInfo":{"status":"ok","timestamp":1654966234370,"user_tz":-540,"elapsed":1,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n","def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","  \n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n","    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n","      tokenized_inputs.append(sentence1)\n","      tokenized_outputs.append(sentence2)\n","  \n","  # 최대 길이 40으로 모든 데이터셋을 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","  \n","  return tokenized_inputs, tokenized_outputs"],"metadata":{"id":"2vWFzLNmktjE","executionInfo":{"status":"ok","timestamp":1654966247511,"user_tz":-540,"elapsed":220,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["questions, answers = tokenize_and_filter(questions, answers)"],"metadata":{"id":"MA9wK0Efktfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('- 단어장의 크기 :',(VOCAB_SIZE))\n","print(f'- 필터링 후 질문의 shape : {questions.shape}')\n","print(f'- 필터링 후 답변의 shape: {answers.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEND9jdErrRj","executionInfo":{"status":"ok","timestamp":1654966369145,"user_tz":-540,"elapsed":250,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"9a358ce4-5466-4024-dd2c-548b16a15271"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["- 단어장의 크기 : 8180\n","- 필터링 후 질문의 shape : (11823, 40)\n","- 필터링 후 답변의 shape: (11823, 40)\n"]}]},{"cell_type":"code","source":["# 인코더와 디코더의 입력, 레이블 만들기\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {'inputs': questions,\n","     'dec_inputs': answers[:, :-1]},  # 디코더의 입력\n","    {'outputs': answers[:, 1:]},      # 시작 토큰 제거\n","))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"metadata":{"id":"tlb0GBVxmyKK","executionInfo":{"status":"ok","timestamp":1654966358003,"user_tz":-540,"elapsed":229,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","### Step 4. 모델 구성하기\n","- 위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."],"metadata":{"id":"YoQu5DViDRhY"}},{"cell_type":"code","source":["# 포지셔널 인코딩 레이어\n","class PositionalEncoding(tf.keras.layers.Layer):\n","\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    # 각도 배열 생성\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","\n","    # 배열의 짝수 인덱스에는 sin 함수 적용\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","    # 배열의 홀수 인덱스에는 cosine 함수 적용\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    # sin과 cosine이 교차되도록 재배열\n","    pos_encoding = tf.stack([sines, cosines], axis=0)\n","    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n","    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n","\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"],"metadata":{"id":"vipahd1RDTlL","executionInfo":{"status":"ok","timestamp":1654966551273,"user_tz":-540,"elapsed":220,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# 스케일드 닷 프로덕트 어텐션 함수\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 가중치를 정규화\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 패딩에 마스크 추가\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # softmax적용\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n","  output = tf.matmul(attention_weights, value)\n","  return output"],"metadata":{"id":"yHvOu0bZkuK5","executionInfo":{"status":"ok","timestamp":1654966553745,"user_tz":-540,"elapsed":224,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# MultiHeadAttention()\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","        super(MultiHeadAttention, self).__init__(name=name)\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.query_dense = tf.keras.layers.Dense(units=d_model)\n","        self.key_dense = tf.keras.layers.Dense(units=d_model)\n","        self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","        self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","    def split_heads(self, inputs, batch_size):\n","        inputs = tf.reshape(\n","            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","            'value'], inputs['mask']\n","        batch_size = tf.shape(query)[0]\n","\n","        # Q, K, V에 각각 Dense를 적용합니다\n","        query = self.query_dense(query)\n","        key = self.key_dense(key)\n","        value = self.value_dense(value)\n","\n","        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n","        query = self.split_heads(query, batch_size)\n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","\n","        # 스케일드 닷 프로덕트 어텐션 함수\n","        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n","        concat_attention = tf.reshape(scaled_attention,\n","                                    (batch_size, -1, self.d_model))\n","\n","        # 최종 결과에도 Dense를 한 번 더 적용합니다\n","        outputs = self.dense(concat_attention)\n","\n","        return outputs"],"metadata":{"id":"wG1S8hCWkuId","executionInfo":{"status":"ok","timestamp":1654966557924,"user_tz":-540,"elapsed":231,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# 패딩 마스킹\n","def create_padding_mask(x):\n","    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","    # (batch_size, 1, 1, sequence length)\n","    return mask[:, tf.newaxis, tf.newaxis, :]"],"metadata":{"id":"l8bpysmRkuGE","executionInfo":{"status":"ok","timestamp":1654966560741,"user_tz":-540,"elapsed":248,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# 룩 어헤드 마스킹\n","def create_look_ahead_mask(x):\n","    seq_len = tf.shape(x)[1]\n","    look_ahead_mask = 1 - \\\n","        tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    padding_mask = create_padding_mask(x)\n","    return tf.maximum(look_ahead_mask, padding_mask)"],"metadata":{"id":"XGwJGotRkuDU","executionInfo":{"status":"ok","timestamp":1654966561752,"user_tz":-540,"elapsed":1,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# encoder_layer()\n","# 인코더 하나의 레이어를 함수로 구현.\n","# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n","def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","    # 패딩 마스크 사용\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n","    attention = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention\")({\n","            'query': inputs,\n","            'key': inputs,\n","            'value': inputs,\n","            'mask': padding_mask\n","        })\n","\n","    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n","    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","    attention = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(inputs + attention)\n","\n","    # 두 번째 서브 레이어 : 2개의 완전연결층\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention + outputs)\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"metadata":{"id":"grxYEI_tkuAs","executionInfo":{"status":"ok","timestamp":1654966562908,"user_tz":-540,"elapsed":243,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# encoder()\n","def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","    # 패딩 마스크 사용\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    # 임베딩 레이어\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","\n","    # 포지셔널 인코딩\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    # num_layers만큼 쌓아올린 인코더의 층.\n","    for i in range(num_layers):\n","        outputs = encoder_layer(\n","            units=units,\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name=\"encoder_layer_{}\".format(i),\n","        )([outputs, padding_mask])\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"metadata":{"id":"5qvhz5ZKneCx","executionInfo":{"status":"ok","timestamp":1654966579169,"user_tz":-540,"elapsed":222,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# decoder_layer()\n","# 디코더 하나의 레이어를 함수로 구현.\n","# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n","def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","    look_ahead_mask = tf.keras.Input(\n","        shape=(1, None, None), name=\"look_ahead_mask\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n","    attention1 = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention_1\")(inputs={\n","            'query': inputs,\n","            'key': inputs,\n","            'value': inputs,\n","            'mask': look_ahead_mask\n","        })\n","\n","    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","    attention1 = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention1 + inputs)\n","\n","    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n","    attention2 = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention_2\")(inputs={\n","            'query': attention1,\n","            'key': enc_outputs,\n","            'value': enc_outputs,\n","            'mask': padding_mask\n","        })\n","\n","    # 마스크드 멀티 헤드 어텐션의 결과는\n","    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","    attention2 = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention2 + attention1)\n","\n","    # 세 번째 서브 레이어 : 2개의 완전연결층\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(outputs + attention2)\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","        outputs=outputs,\n","        name=name)"],"metadata":{"id":"eLrNd6bjneA1","executionInfo":{"status":"ok","timestamp":1654966591144,"user_tz":-540,"elapsed":251,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# decoder()\n","def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","    inputs = tf.keras.Input(shape=(None,), name='inputs')\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","    look_ahead_mask = tf.keras.Input(\n","        shape=(1, None, None), name='look_ahead_mask')\n","\n","    # 패딩 마스크\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    # 임베딩 레이어\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","\n","    # 포지셔널 인코딩\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    # Dropout이라는 훈련을 돕는 테크닉을 수행\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    for i in range(num_layers):\n","        outputs = decoder_layer(\n","            units=units,\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name='decoder_layer_{}'.format(i),\n","        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","        outputs=outputs,\n","        name=name)"],"metadata":{"id":"LGGep5DOnd-4","executionInfo":{"status":"ok","timestamp":1654966597592,"user_tz":-540,"elapsed":219,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# trainsformer()\n","def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","    # 인코더에서 패딩을 위한 마스크\n","    enc_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask,\n","        output_shape=(1, 1, None),\n","        name='enc_padding_mask')(inputs)\n","\n","    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n","    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n","    look_ahead_mask = tf.keras.layers.Lambda(\n","        create_look_ahead_mask,\n","        output_shape=(1, None, None),\n","        name='look_ahead_mask')(dec_inputs)\n","\n","    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n","    # 디코더에서 패딩을 위한 마스크\n","    dec_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask,\n","        output_shape=(1, 1, None),\n","        name='dec_padding_mask')(inputs)\n","\n","    # 인코더\n","    enc_outputs = encoder(\n","        vocab_size=vocab_size,\n","        num_layers=num_layers,\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","    )(inputs=[inputs, enc_padding_mask])\n","\n","    # 디코더\n","    dec_outputs = decoder(\n","        vocab_size=vocab_size,\n","        num_layers=num_layers,\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","    # 완전연결층\n","    outputs = tf.keras.layers.Dense(\n","        units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"],"metadata":{"id":"YdNN_Yl2nd8_","executionInfo":{"status":"ok","timestamp":1654966635457,"user_tz":-540,"elapsed":223,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","tf.keras.backend.clear_session()\n","\n","# 하이퍼파라미터\n","NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n","D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n","NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n","UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n","DROPOUT = 0.1 # 드롭아웃의 비율\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    units=UNITS,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-YH0MtJnd45","executionInfo":{"status":"ok","timestamp":1654968394414,"user_tz":-540,"elapsed":2637,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"e0d23f27-4500-4955-fcc5-2c1583a1eb8e"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," inputs (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," dec_inputs (InputLayer)        [(None, None)]       0           []                               \n","                                                                                                  \n"," enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," encoder (Functional)           (None, None, 256)    3148288     ['inputs[0][0]',                 \n","                                                                  'enc_padding_mask[0][0]']       \n","                                                                                                  \n"," look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n","                                e)                                                                \n","                                                                                                  \n"," dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," decoder (Functional)           (None, None, 256)    3675648     ['dec_inputs[0][0]',             \n","                                                                  'encoder[0][0]',                \n","                                                                  'look_ahead_mask[0][0]',        \n","                                                                  'dec_padding_mask[0][0]']       \n","                                                                                                  \n"," outputs (Dense)                (None, None, 8180)   2102260     ['decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 8,926,196\n","Trainable params: 8,926,196\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# 손실 함수\n","def loss_function(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')(y_true, y_pred)\n","\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","    loss = tf.multiply(loss, mask)\n","\n","    return tf.reduce_mean(loss)"],"metadata":{"id":"q0rTg5Xznd2x","executionInfo":{"status":"ok","timestamp":1654968397978,"user_tz":-540,"elapsed":1,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# 커스텀 학습률\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"d3Yo142Snd0o","executionInfo":{"status":"ok","timestamp":1654968399886,"user_tz":-540,"elapsed":2,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# 커스텀 된 학습률을 사용하여 모델 컴파일\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"],"metadata":{"id":"x44sHd3KndyK","executionInfo":{"status":"ok","timestamp":1654968404598,"user_tz":-540,"elapsed":234,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# 에폭 30으로 학습\n","EPOCHS = 30\n","model.fit(dataset, epochs=EPOCHS, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfiDYuQczfWw","executionInfo":{"status":"ok","timestamp":1654970120323,"user_tz":-540,"elapsed":1675501,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"2a917fc0-80f7-4c09-a474-50f986cc0b2e"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","185/185 [==============================] - 62s 300ms/step - loss: 1.4622 - accuracy: 0.0282\n","Epoch 2/30\n","185/185 [==============================] - 56s 301ms/step - loss: 1.1837 - accuracy: 0.0494\n","Epoch 3/30\n","185/185 [==============================] - 57s 306ms/step - loss: 1.0065 - accuracy: 0.0506\n","Epoch 4/30\n","185/185 [==============================] - 57s 307ms/step - loss: 0.9293 - accuracy: 0.0544\n","Epoch 5/30\n","185/185 [==============================] - 56s 305ms/step - loss: 0.8703 - accuracy: 0.0578\n","Epoch 6/30\n","185/185 [==============================] - 57s 308ms/step - loss: 0.8089 - accuracy: 0.0623\n","Epoch 7/30\n","185/185 [==============================] - 56s 305ms/step - loss: 0.7421 - accuracy: 0.0684\n","Epoch 8/30\n","185/185 [==============================] - 57s 306ms/step - loss: 0.6691 - accuracy: 0.0761\n","Epoch 9/30\n","185/185 [==============================] - 56s 305ms/step - loss: 0.5900 - accuracy: 0.0846\n","Epoch 10/30\n","185/185 [==============================] - 56s 304ms/step - loss: 0.5069 - accuracy: 0.0943\n","Epoch 11/30\n","185/185 [==============================] - 57s 306ms/step - loss: 0.4244 - accuracy: 0.1044\n","Epoch 12/30\n","185/185 [==============================] - 57s 309ms/step - loss: 0.3437 - accuracy: 0.1155\n","Epoch 13/30\n","185/185 [==============================] - 56s 304ms/step - loss: 0.2698 - accuracy: 0.1263\n","Epoch 14/30\n","185/185 [==============================] - 56s 301ms/step - loss: 0.2048 - accuracy: 0.1362\n","Epoch 15/30\n","185/185 [==============================] - 56s 302ms/step - loss: 0.1507 - accuracy: 0.1455\n","Epoch 16/30\n","185/185 [==============================] - 56s 302ms/step - loss: 0.1090 - accuracy: 0.1533\n","Epoch 17/30\n","185/185 [==============================] - 57s 306ms/step - loss: 0.0794 - accuracy: 0.1588\n","Epoch 18/30\n","185/185 [==============================] - 56s 303ms/step - loss: 0.0618 - accuracy: 0.1619\n","Epoch 19/30\n","185/185 [==============================] - 55s 299ms/step - loss: 0.0507 - accuracy: 0.1639\n","Epoch 20/30\n","185/185 [==============================] - 54s 292ms/step - loss: 0.0458 - accuracy: 0.1645\n","Epoch 21/30\n","185/185 [==============================] - 54s 294ms/step - loss: 0.0432 - accuracy: 0.1648\n","Epoch 22/30\n","185/185 [==============================] - 54s 293ms/step - loss: 0.0402 - accuracy: 0.1655\n","Epoch 23/30\n","185/185 [==============================] - 55s 298ms/step - loss: 0.0367 - accuracy: 0.1662\n","Epoch 24/30\n","185/185 [==============================] - 55s 295ms/step - loss: 0.0320 - accuracy: 0.1673\n","Epoch 25/30\n","185/185 [==============================] - 54s 293ms/step - loss: 0.0276 - accuracy: 0.1685\n","Epoch 26/30\n","185/185 [==============================] - 54s 293ms/step - loss: 0.0253 - accuracy: 0.1691\n","Epoch 27/30\n","185/185 [==============================] - 54s 293ms/step - loss: 0.0220 - accuracy: 0.1699\n","Epoch 28/30\n","185/185 [==============================] - 55s 297ms/step - loss: 0.0204 - accuracy: 0.1702\n","Epoch 29/30\n","185/185 [==============================] - 54s 294ms/step - loss: 0.0187 - accuracy: 0.1706\n","Epoch 30/30\n","185/185 [==============================] - 54s 294ms/step - loss: 0.0171 - accuracy: 0.1711\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f03373aa710>"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["<br>\n","\n","### Step 5. 모델 평가하기\n","- Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."],"metadata":{"id":"tWCLEMMEyGRC"}},{"cell_type":"code","source":["def decoder_inference(sentence):\n","    sentence = preprocess_sentence(sentence)\n","\n","    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n","    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n","    sentence = tf.expand_dims(\n","        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n","    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n","    output_sequence = tf.expand_dims(START_TOKEN, 0)\n","\n","    # 디코더의 인퍼런스 단계\n","    for i in range(MAX_LENGTH):\n","        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n","        predictions = model(inputs=[sentence, output_sequence], training=False)\n","        predictions = predictions[:, -1:, :]\n","\n","        # 현재 예측한 단어의 정수\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n","        if tf.equal(predicted_id, END_TOKEN[0]):\n","            break\n","\n","        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n","        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n","        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output_sequence, axis=0)"],"metadata":{"id":"EuK6FGM5kunF","executionInfo":{"status":"ok","timestamp":1654970129316,"user_tz":-540,"elapsed":221,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def sentence_generation(sentence):\n","    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n","    prediction = decoder_inference(sentence)\n","\n","    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n","    predicted_sentence = tokenizer.decode(\n","        [i for i in prediction if i < tokenizer.vocab_size])\n","\n","    print('입력 : {}'.format(sentence))\n","    print('출력 : {}'.format(predicted_sentence))\n","\n","    return predicted_sentence"],"metadata":{"id":"0Itwc6Uykukc","executionInfo":{"status":"ok","timestamp":1654970131531,"user_tz":-540,"elapsed":219,"user":{"displayName":"진샛별","userId":"17569353266716707988"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["sentence_generation('너는 누구니')"],"metadata":{"id":"0I6bHx8LyHew","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1654970191731,"user_tz":-540,"elapsed":576,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"48ae1ec1-6501-41dc-906b-4e02768b08e2"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 너는 누구니\n","출력 : 저는 위로해드리는 로봇이에요 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'저는 위로해드리는 로봇이에요 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["sentence_generation('자기소개 할 줄 아니')"],"metadata":{"id":"WYG2s5SfyHw3","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1654970204361,"user_tz":-540,"elapsed":674,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"3a8601b9-d318-4efe-f324-d22de97ecca6"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 자기소개 할 줄 아니\n","출력 : 누구든 힘들어할 거예요 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'누구든 힘들어할 거예요 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["sentence_generation('나이를 물어봐도 되나?')"],"metadata":{"id":"cCoTe8KqyIBw","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1654970242647,"user_tz":-540,"elapsed":830,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"accc90f6-e91e-43a2-ac2c-fdb86d287243"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 나이를 물어봐도 되나?\n","출력 : 연락처 물어보는 건 전세계 공통이에요 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'연락처 물어보는 건 전세계 공통이에요 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["sentence_generation('연락처는?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"g0XcfkqHokHG","executionInfo":{"status":"ok","timestamp":1654970259896,"user_tz":-540,"elapsed":1173,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"56d5f0b6-dbc5-48c2-84a5-1c1d73682c17"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 연락처는?\n","출력 : 썸 타다가 연인 되는 게 일반적이죠 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'썸 타다가 연인 되는 게 일반적이죠 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["sentence_generation('썸은 뭐니')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"a2v_GrpmollG","executionInfo":{"status":"ok","timestamp":1654970272359,"user_tz":-540,"elapsed":586,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"a1ae35f7-b753-427a-a739-449c696921ed"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 썸은 뭐니\n","출력 : 연락도 하고 직접 만나보세요 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'연락도 하고 직접 만나보세요 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["sentence_generation('내일은 어떨까?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"CLs4qLu_olio","executionInfo":{"status":"ok","timestamp":1654970304857,"user_tz":-540,"elapsed":595,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"e37f84c9-b119-49d9-c03a-d79ab99d52f4"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 내일은 어떨까?\n","출력 : 너무 낙담하지 마세요 .\n"]},{"output_type":"execute_result","data":{"text/plain":["'너무 낙담하지 마세요 .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["sentence_generation('내일은 괜찮을까?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"M77iU-7folgv","executionInfo":{"status":"ok","timestamp":1654970318518,"user_tz":-540,"elapsed":845,"user":{"displayName":"진샛별","userId":"17569353266716707988"}},"outputId":"d272d44f-610a-4c2e-9d49-1d440a5d3d17"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 내일은 괜찮을까?\n","출력 : 기우제를 지내봅시다 !\n"]},{"output_type":"execute_result","data":{"text/plain":["'기우제를 지내봅시다 !'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","source":["## ✔️회고 및 결론\n","---"],"metadata":{"id":"PPyKwDJpEUGD"}},{"cell_type":"markdown","source":["#### ▶️ 이번 프로젝트에서 어려웠던 점\n","\n","예제는 영어였는데 프로젝트 자체는 한국어 임베딩이라는 점에서 매 순간마다 신경이 곤두섰던 기억이 난다. 예전에 요약문 만들때 문법이 뒤죽박죽으로 엉켜서 고생했던 적이 있어서 토크나이징 때부터 과정을 신경썼는데 그래도 이번에는 반타작은 한 것 같다.\n","\n","<br>\n","\n","#### ▶️ 프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점\n","\n","생각보다 학습할 당시에 30에폭 만으로도 엄청나게 로스값이 떨어지는 것을 확인할 수 있었는데 그와 상반되게 정확도는 0.2조차 되지 않는다는 점이었다. 이게 구체적으로 어떤 기준인지, 왜 이렇게 잘 오르지 않았는데도 학습이 됐는지의 여부를 좀 더 파고들어야 제대로 NLP의 학습과정에 집중할 수 있겠다는 생각이 들었다.\n","\n","<br>\n"],"metadata":{"id":"qW8CrbzLY_wt"}},{"cell_type":"markdown","source":["#### ▶️ 루브릭 평가 지표를 맞추기 위해 시도한 것들\n","\n","이번 프로젝트에서 루브릭의 기준은 아래와 같았다."],"metadata":{"id":"U4xO9O9AEXrN"}},{"cell_type":"markdown","source":["|       |                                    **< 평가문항 >**                                   |                                                              **< 상세기준 >**                                                             |\n","|-------|:---------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|\n","| **1** | 한국어 전처리를 통해 학습 데이터셋을 구축하였다.                                 | 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.<br>                                                        |\n","| **2** | 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다. | 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.                              |\n","| **3** | 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.                                    | 한국어 입력문장에 그럴듯한 한국어로 답변을 리턴하였다. |"],"metadata":{"id":"hTkfLutpY67j"}},{"cell_type":"markdown","source":["LMS상의 영어 학습을 기반으로 한글을 구축한건데, 염려와는 다르게 토크나이징과 임베딩 단계에서부터 심심이 봇처럼 챗봇의 말이 어느정도 이어진다는 것을 확인할 수 있었다. 따라서 루브릭의 충족사항들은 모두 지킨 것 같다.\n","\n","\n","<br>\n","\n","#### ▶️ 자기 다짐\n","\n","NLP의 영역은 언제봐도 심오한 것 같다. 워낙 글이나 언어에 대한 관심이 많은 편이라서 기본적인 흥미는 있지만, 잘하는 것과 좋아하는 것의 차이는 엄연히 다르다고 생각한다. NLP는 아무래도 시간을 좀더 들여서 들여다봐야하는 분야인 것 같다. 결국 CV를 위주로 하더라도 OCR같은 다음 파트같은 경우 NLP의 영역도 다루게 될 것이기에 어느 하나에만 집중한다는건 쉽지 않다고 생각한다.\n","\n","<br>"],"metadata":{"id":"WvWXBqdMwuHI"}},{"cell_type":"markdown","source":["## ✔️Reference(참고자료)\n","---\n","* https://wikidocs.net/89786\n","* https://github.com/mochafreddo/aiffel/blob/b7c625bf3613f3f6deac85faa3d136743b89aca6/%5BE-15%5DCreate%20a%20chatbot%20with%20Korean%20data.ipynb\n","* https://tonyaround.com/%ec%b1%97%eb%b4%87-%ea%b8%b0%ed%9a%8d-%eb%8b%a8%ea%b3%84-%ec%b1%97%eb%b4%87%ec%9d%98-5%ea%b0%80%ec%a7%80-%eb%8c%80%ed%91%9c-%ec%9c%a0%ed%98%95-%ec%a2%85%eb%a5%98/\n"],"metadata":{"id":"9DGVhS4BEYHN"}}]}